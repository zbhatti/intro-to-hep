{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs To 4 Leptons Analysis with Systematic Uncertainties Using uproot, awkward and pyhf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Variable Initialization\n",
    "\n",
    "This is where we state the location of our .root files. To reduce the memory usage, we select which columns of the .root files will be used when building our dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original: https://root.cern.ch/doc/v622/df106__HiggsToFourLeptons_8py.html\n",
    "from collections import Counter\n",
    "from itertools import compress\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from pprint import pprint as pp\n",
    "\n",
    "from datetime import datetime\n",
    "t1 = datetime.now()\n",
    "print(t1)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import uproot4 as uproot\n",
    "from uproot_methods import TLorentzVector\n",
    "import awkward1 as ak\n",
    "\n",
    "path = \"root://eospublic.cern.ch//eos/opendata/atlas/OutreachDatasets/2020-01-22\"\n",
    "path = \"/home/zbhatti/codebase/intro-to-hep/OutreachDatasets/2020-01-22\"\n",
    "files = json.load(open(os.path.join(os.environ[\"ROOTSYS\"], \"tutorials/dataframe\", \"df106_HiggsToFourLeptons.json\")))\n",
    "\n",
    "pp(files)\n",
    "use_cached = True\n",
    "lumi = 10064.0\n",
    "\n",
    "read_columns = ['trigE', 'trigM', 'lep_eta', 'lep_pt', 'lep_ptcone30', 'lep_etcone20', \n",
    "                'lep_type', 'lep_charge', 'lep_phi', 'lep_E', 'lep_trackd0pvunbiased', \n",
    "                'lep_tracksigd0pvunbiased', 'lep_z0', 'mcWeight', 'scaleFactor_ELE', \n",
    "                'scaleFactor_MUON', 'scaleFactor_LepTRIGGER', 'scaleFactor_PILEUP',\n",
    "               ]\n",
    "\n",
    "dataframes_npy_filename = 'higgs_4l_dataframes-{}-.npy'\n",
    "cut_counts_npy_filename = 'higgs_4l_cut_counts-{}-.npy'\n",
    "\n",
    "processes = files.keys()\n",
    "dataframes = {}\n",
    "dataframes_high = {}\n",
    "dataframes_low = {}\n",
    "xsecs = {}\n",
    "sumws = {}\n",
    "samples = []\n",
    "cut_counts = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load .root Files and Apply Cuts\n",
    "\n",
    "Here we apply cuts, reducing the total number of events we eventually bin into histograms. Using existing or defining new columns helps us make the right choice of filters to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorentz_vector(pt, eta, phi, e):\n",
    "    return TLorentzVector.from_ptetaphie(pt, eta, phi, e)\n",
    "\n",
    "# https://docs.python.org/3/whatsnew/3.5.html#whatsnew-pep-448 for [*map()] behavior\n",
    "def lorentz_vectors(pt_arr, eta_arr, phi_arr, e_arr):\n",
    "    return [*map(TLorentzVector.from_ptetaphie, pt_arr, eta_arr, phi_arr, e_arr)]\n",
    "\n",
    "# cuts based on lepton flavor\n",
    "def good_electrons_and_muons(pdg_id_arr, lv_arr, trackd0pv_arr, tracksigd0pv_arr, z0_arr):\n",
    "    for i, pdg_id in enumerate(pdg_id_arr):\n",
    "        if pdg_id == 11:\n",
    "            if (lv_arr[i].pt < 7000 or abs(lv_arr[i].eta) > 2.47 or abs(trackd0pv_arr[i] / tracksigd0pv_arr[i]) > 5 or abs(z0_arr[i] * math.sin(lv_arr[i].theta)) > 0.5):\n",
    "                return False\n",
    "        else:\n",
    "            if (abs(trackd0pv_arr[i] / tracksigd0pv_arr[i]) > 5 or abs(z0_arr[i] * math.sin(lv_arr[i].theta)) > 0.5):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# works on ALL leptons in all events, or per event\n",
    "def good_lep(lep_eta, lep_pt, lep_ptcone30, lep_etcone20):\n",
    "    a = np.logical_and(abs(lep_eta) < 2.5, lep_pt > 5000)\n",
    "    b = np.logical_and(lep_ptcone30 / lep_pt < 0.3, lep_etcone20 / lep_pt < 0.3)\n",
    "    return np.logical_and(a, b)\n",
    "    \n",
    "def invariant_mass(lep_lv_arr):\n",
    "    return math.sqrt((lep_lv_arr[0] + lep_lv_arr[1] + lep_lv_arr[2] + lep_lv_arr[3]).mass2) / 1000.0;\n",
    "\n",
    "def build_dataframe(tree, xsecs_sample, sumws_sample, data_or_mc, threshold):\n",
    "    dataframes = None\n",
    "    cut_counts = np.array([0, 0, 0, 0, 0, 0, 0])\n",
    "    \n",
    "    # Dummy experimental uncertainty \n",
    "    if 'high' == threshold:\n",
    "        pt_threshold = 12000\n",
    "    elif 'low' == threshold:\n",
    "        pt_threshold = 8000\n",
    "    else:\n",
    "        pt_threshold = 10000\n",
    "    \n",
    "    for df in tree.iterate(read_columns, library='ak'):\n",
    "            sample_batch_counts = [df.shape[0]]\n",
    "            \n",
    "            # Filter 1\n",
    "            df = df[df.trigE | df.trigM]\n",
    "            sample_batch_counts.append(df.shape[0])\n",
    "            \n",
    "            # can call on ALL events\n",
    "            df['good_lep'] = good_lep(df.lep_eta, df.lep_pt, df.lep_ptcone30, df.lep_etcone20)\n",
    "            \n",
    "            # or PER event (slower)\n",
    "            # df['good_lep'] = [*map(good_lep, df.lep_eta, df.lep_pt, df.lep_ptcone30, df.lep_etcone20)]\n",
    "\n",
    "            # Filter 2\n",
    "            df = df[np.sum(df.good_lep, axis=1) == 4]\n",
    "            sample_batch_counts.append(df.shape[0])\n",
    "            \n",
    "            # Filter 3\n",
    "            df = df[np.sum(df.lep_charge[df.good_lep], axis=1) == 0]\n",
    "            sample_batch_counts.append(df.shape[0])\n",
    "\n",
    "            df['goodlep_sumtypes'] = np.sum(df.lep_type[df.good_lep], axis=1)\n",
    "            \n",
    "            # Filter 4\n",
    "            df = df[(df.goodlep_sumtypes == 44) | (df.goodlep_sumtypes == 52) | (df.goodlep_sumtypes == 48)]\n",
    "            sample_batch_counts.append(df.shape[0])\n",
    "            \n",
    "            df['goodlep_pt'] = df.lep_pt[df.good_lep]\n",
    "            df['goodlep_eta'] = df.lep_eta[df.good_lep]\n",
    "            df['goodlep_phi'] = df.lep_phi[df.good_lep]\n",
    "            df['goodlep_E'] = df.lep_E[df.good_lep]\n",
    "\n",
    "            goodlep_lv = [*map(lorentz_vectors, \n",
    "                                ak.to_numpy(df.goodlep_pt),\n",
    "                                ak.to_numpy(df.goodlep_eta),\n",
    "                                ak.to_numpy(df.goodlep_phi),\n",
    "                                ak.to_numpy(df.goodlep_E)\n",
    "                               )]\n",
    "\n",
    "            # Filter 5           \n",
    "            good_e_mu_filter = [*map(good_electrons_and_muons,\n",
    "                                     ak.to_numpy(df.lep_type[df.good_lep]),\n",
    "                                     goodlep_lv,\n",
    "                                     ak.to_numpy(df.lep_trackd0pvunbiased[df.good_lep]),\n",
    "                                     ak.to_numpy(df.lep_tracksigd0pvunbiased[df.good_lep]),\n",
    "                                     ak.to_numpy(df.lep_z0[df.good_lep])\n",
    "                                    )]\n",
    "            df = df[good_e_mu_filter]\n",
    "            good_e_mu_lv = [*compress(goodlep_lv, good_e_mu_filter)]\n",
    "            sample_batch_counts.append(df.shape[0])\n",
    "            \n",
    "            # Filter 6\n",
    "            high_pt_filter = [*map(lambda x: x[0] > 25000 and x[1] > 15000 and x[2] > pt_threshold, df.goodlep_pt)]\n",
    "            df = df[high_pt_filter]\n",
    "            high_pt_lv = [*compress(good_e_mu_lv, high_pt_filter)]\n",
    "\n",
    "            sample_batch_counts.append(df.shape[0])\n",
    "            print(sample_batch_counts)\n",
    "            \n",
    "            df['m4l'] = [*map(invariant_mass, high_pt_lv)]\n",
    "                        \n",
    "            if \"data\" in data_or_mc:\n",
    "                df['weight'] = np.ones(df.shape[0]) # DOUBLE CHECK THESE ARE FLOATS!\n",
    "            \n",
    "            elif \"mc\" in data_or_mc:\n",
    "                df['weight'] = df.scaleFactor_ELE * df.scaleFactor_MUON * \\\n",
    "                               df.scaleFactor_LepTRIGGER * df.scaleFactor_PILEUP * \\\n",
    "                               df.mcWeight * (xsecs_sample / sumws_sample) * lumi\n",
    "            \n",
    "            # free up memory       \n",
    "            df = df[['m4l', 'weight']]\n",
    "            cut_counts = np.array(sample_batch_counts) + cut_counts\n",
    "            \n",
    "            if dataframes is None: \n",
    "                dataframes = df\n",
    "                \n",
    "            else:\n",
    "                dataframes = dataframes.append(df) \n",
    "        \n",
    "    return dataframes, cut_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in processes:\n",
    "    for d in files[p]:\n",
    "        # Construct the dataframes\n",
    "        folder = d[0] # Folder name\n",
    "        sample = d[1] # Sample name\n",
    "        xsecs[sample] = d[2] # Cross-section\n",
    "        sumws[sample] = d[3] # Sum of weights\n",
    "        samples.append(sample)\n",
    "        filepath = \"{}/4lep/{}/{}.4lep.root\".format(path, folder, sample)\n",
    "        print(sample, filepath)\n",
    "        uproot_file = uproot.open(filepath)\n",
    "        tree = uproot_file['mini']\n",
    "        data_or_mc = \"data\" if \"data\" in sample.lower() else \"mc\"\n",
    "        \n",
    "        if not use_cached:\n",
    "            dataframes[sample], cut_counts[sample] = build_dataframe(tree, xsecs[sample], sumws[sample], data_or_mc, None)\n",
    "            dataframes_high[sample], _ = build_dataframe(tree, xsecs[sample], sumws[sample], data_or_mc, 'high')\n",
    "            dataframes_low[sample], _ = build_dataframe(tree, xsecs[sample], sumws[sample], data_or_mc, 'low')\n",
    "            np.save(dataframes_npy_filename.format(sample), ak.to_numpy(dataframes[sample]))\n",
    "            np.save(dataframes_npy_filename.format(sample+'.high'), ak.to_numpy(dataframes_high[sample]))\n",
    "            np.save(dataframes_npy_filename.format(sample+'.low'), ak.to_numpy(dataframes_low[sample]))\n",
    "            np.save(cut_counts_npy_filename.format(sample), cut_counts[sample])\n",
    "            \n",
    "            del dataframes[sample]\n",
    "            del dataframes_high[sample]\n",
    "            del dataframes_low[sample]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Previous Run of the Cuts\n",
    "\n",
    "Fortunately, the work done in the previous steps are a checkpoint. We can pick up our analysis here if we do not need to update the cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_filenames = glob.glob(dataframes_npy_filename.format('*'))\n",
    "if use_cached:\n",
    "    for filename in pickle_filenames:\n",
    "        sample = filename.split('-')[2]\n",
    "        print('loading sample {}'.format(sample))\n",
    "        if sample.endswith('.high'):\n",
    "            dataframes_high[sample[:-5]] = np.load(filename)\n",
    "        elif sample.endswith('.low'):\n",
    "            dataframes_low[sample[:-4]] = np.load(filename)\n",
    "        else:\n",
    "            dataframes[sample] = np.load(filename)\n",
    "        cut_counts[sample] = np.load(cut_counts_pickle_filename.format(sample))\n",
    "        samples.append(sample)\n",
    "\n",
    "print('Final Shapes for Samples:')\n",
    "pp(cut_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency of the Cuts\n",
    "\n",
    "Here we see how the cuts reduce background events while not excluding potential signal events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_efficiency = pd.DataFrame()\n",
    "\n",
    "# Merge Signal and BG counts\n",
    "cut_efficiency['Signal N'] = cut_counts['mc_345060.ggH125_ZZ4lep'] + cut_counts['mc_344235.VBFH125_ZZ4lep']\n",
    "cut_efficiency['Background N'] = cut_counts['mc_361106.Zee'] + cut_counts['mc_361107.Zmumu'] + cut_counts['mc_363490.llll']\n",
    "cut_efficiency['Signal %'] = np.full((cut_counts['mc_361106.Zee'].shape[0], ), np.nan)\n",
    "cut_efficiency['Background %'] = np.full((cut_counts['mc_361106.Zee'].shape[0], ), np.nan)\n",
    "\n",
    "# Produce fractional numbers\n",
    "for region_name in ['Signal', 'Background']:\n",
    "    region_pct = region_name + ' %'\n",
    "    region_n = region_name + ' N'\n",
    "    for filter_i, count in enumerate(cut_efficiency[region_n]):\n",
    "    \n",
    "        if filter_i == 0:\n",
    "            continue\n",
    "        else:\n",
    "            prev = cut_efficiency[region_n][filter_i-1]\n",
    "            curr = cut_efficiency[region_n][filter_i]\n",
    "            cut_efficiency[region_pct][filter_i] = abs(prev - curr) / prev * 100.\n",
    "\n",
    "print('events passing each cut and percent difference from prev filter')\n",
    "pp(cut_efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Samples Into Background, Signal and Data Sets\n",
    "\n",
    "Initially, our labeled data sets are split across multiple .root files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with bin, low and high parameters taken from first histogram\n",
    "def merge_series(label):\n",
    "    x = None\n",
    "    weights = None\n",
    "    for i, d in enumerate(files[label]):\n",
    "        sample = d[1]\n",
    "        print('merging {}...'.format(sample))\n",
    "        sample_x = dataframes[sample].m4l.to_numpy()\n",
    "        sample_weights = dataframes[sample].weight.to_numpy()\n",
    "        if i == 0: \n",
    "            x = sample_x\n",
    "            weights = sample_weights\n",
    "        else: \n",
    "            x = np.concatenate((x, sample_x), axis=None)\n",
    "            weights = np.concatenate((weights, sample_weights), axis=None)\n",
    "    return x, weights\n",
    "\n",
    "data_x, data_weights = merge_series(\"data\")\n",
    "higgs_x, higgs_weights = merge_series(\"higgs\")\n",
    "zz_x, zz_weights = merge_series(\"zz\")\n",
    "other_x, other_weights = merge_series(\"other\")\n",
    "\n",
    "dataframes = {} # free up some precious memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Histograms\n",
    "\n",
    "Now that all our labeled sets are complete, we can start building histograms and finding the statistical uncertainties for each bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 24\n",
    "bin_range = (80, 170)\n",
    "bin_edges = np.linspace(bin_range[0], bin_range[1], n_bins)\n",
    "center_offset = (bin_range[1] - bin_range[0]) / n_bins /2. \n",
    "bin_centers = np.linspace(bin_range[0]+center_offset, bin_range[1]-center_offset, n_bins)\n",
    "\n",
    "def extract_stat_uncertainty(x, x_weights, x_hist):\n",
    "    # get indices of which bins x belong to; np.digitize() returns bin id rather than bin index position\n",
    "    x_bin_ids = np.digitize(x, bins=x_hist[1]) \n",
    "    \n",
    "    weights_by_bin = [np.array([0.,]),]*len(x_hist[0]) # an empty bin has 0.0 stat uncertainty\n",
    "    \n",
    "    # first group weights by their corresponding x bins\n",
    "    for i, bin_id in enumerate(x_bin_ids):\n",
    "        if bin_id == 0 or bin_id == n_bins+1: # less than left edge or larger than right edge\n",
    "            continue\n",
    "        bin_index = bin_id - 1\n",
    "        corresponding_weight = x_weights[i]\n",
    "        if np.sum(weights_by_bin[bin_index]) == 0.:\n",
    "            weights_by_bin[bin_index] = np.array([corresponding_weight, ])\n",
    "        \n",
    "        else:\n",
    "            weights_by_bin[bin_index] = np.concatenate((weights_by_bin[bin_index], np.array([corresponding_weight, ])), axis=None)\n",
    "    \n",
    "    # now calculate statistical uncertainty per bin\n",
    "    return np.array([np.sqrt(np.sum(bin_weights**2)) for bin_weights in weights_by_bin])\n",
    "\n",
    "data_hist = np.histogram(data_x, bins=n_bins, range=bin_range, weights=data_weights)\n",
    "higgs_hist = np.histogram(higgs_x, bins=n_bins, range=bin_range, weights=higgs_weights)\n",
    "zz_hist = np.histogram(zz_x, bins=n_bins, range=bin_range, weights=zz_weights)\n",
    "other_hist = np.histogram(other_x, bins=n_bins, range=bin_range, weights=other_weights)\n",
    "\n",
    "higgs_hist_err = extract_stat_uncertainty(higgs_x, higgs_weights, higgs_hist)\n",
    "zz_hist_err = extract_stat_uncertainty(zz_x, zz_weights, zz_hist)\n",
    "other_hist_err = extract_stat_uncertainty(other_x, other_weights, other_hist)\n",
    "\n",
    "print('higgs, zz, other stat errors:')\n",
    "pp(higgs_hist_err)\n",
    "pp(zz_hist_err)\n",
    "pp(other_hist_err)\n",
    "\n",
    "print('Histograms:')\n",
    "print('bins:')\n",
    "pp(bin_edges)\n",
    "\n",
    "print('data:')\n",
    "pp(data_hist[0])\n",
    "\n",
    "print('higgs:')\n",
    "pp(higgs_hist[0])\n",
    "\n",
    "print('zz:')\n",
    "# Apply MC correction for ZZ due to missing gg->ZZ process\n",
    "zz_hist = zz_hist[0]*1.3, zz_hist[1]\n",
    "pp(zz_hist[0])\n",
    "\n",
    "print('other:')\n",
    "pp(other_hist[0])\n",
    "\n",
    "print('comparing data to mc hist')\n",
    "pp(other_hist[0] + zz_hist[0] + higgs_hist[0])\n",
    "pp(data_hist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Invariant Mass Histogram\n",
    "\n",
    "See how the Monte Carlo generated counts compare to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stacked_hist(zz_factor, higgs_factor):\n",
    "    main_axes = plt.gca()\n",
    "    main_axes.errorbar(x=bin_centers, y=data_hist[0], yerr=np.sqrt(data_hist[0]), fmt='ko', label='data')\n",
    "\n",
    "    # https://matplotlib.org/3.3.0/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "    # https://matplotlib.org/3.3.0/gallery/statistics/histogram_multihist.html\n",
    "\n",
    "    main_axes.hist(x=[other_x, zz_x, higgs_x, ], \n",
    "                   bins=n_bins, \n",
    "                   range=bin_range, \n",
    "                   weights=[other_weights, zz_weights*zz_factor, higgs_weights*higgs_factor], \n",
    "                   color=['mediumpurple', 'skyblue', 'red'], \n",
    "                   label=['other', 'zz', 'higgs'],\n",
    "                   histtype='bar',\n",
    "                   stacked=True\n",
    "                  )\n",
    "\n",
    "    plt.xlim(bin_range[0], bin_range[1])\n",
    "    plt.ylim(0, 35)\n",
    "    plt.xlabel('m_4l H->ZZ [GeV]')\n",
    "    plt.ylabel('Events')\n",
    "    plt.xticks(np.arange(bin_range[0], bin_range[1]+10, 10))\n",
    "    plt.legend()\n",
    "\n",
    "plot_stacked_hist(1.3, 1.0)\n",
    "\n",
    "t2 = datetime.now()\n",
    "print(t2)\n",
    "print('total runtime: {} s'.format((t2-t1).seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the pyhf Workspace\n",
    "\n",
    "Here we define the workspace which will eventually be used by pyhf to give the signal region normalization factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = None\n",
    "pyhf_workspace = \\\n",
    "{\n",
    "    \"channels\": [\n",
    "        {\n",
    "            \"name\": \"Signal_region\",\n",
    "            \"samples\": [\n",
    "                {\n",
    "                    \"data\": zz_hist[0].tolist(),\n",
    "                    \"modifiers\": [\n",
    "                        {\n",
    "                            \"data\": zz_hist_err.tolist(),\n",
    "                            \"name\": \"staterror_Signal_region\",\n",
    "                            \"type\": \"staterror\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"name\": \"ZZ\"\n",
    "                },\n",
    "                {\n",
    "                    \"data\": higgs_hist[0].tolist(),\n",
    "                    \"modifiers\": [\n",
    "                        {\n",
    "                            \"data\": higgs_hist_err.tolist(),\n",
    "                            \"name\": \"staterror_Signal_region\",\n",
    "                            \"type\": \"staterror\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"data\": null,\n",
    "                            \"name\": \"Signal_norm\",\n",
    "                            \"type\": \"normfactor\" # free parameter in fit\n",
    "                        }\n",
    "                    ],\n",
    "                    \"name\": \"Higgs\"\n",
    "                },\n",
    "                {\n",
    "                    \"data\": other_hist[0].tolist(),\n",
    "                    \"modifiers\": [\n",
    "                        {\n",
    "                            \"data\": other_hist_err.tolist(),\n",
    "                            \"name\": \"staterror_Signal_region\",\n",
    "                            \"type\": \"staterror\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"name\": \"Other\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"measurements\": [\n",
    "        {\n",
    "            \"config\": {\n",
    "                \"parameters\": [\n",
    "                    {\n",
    "                        \"bounds\": [\n",
    "                            [\n",
    "                                0,\n",
    "                                5\n",
    "                            ]\n",
    "                        ],\n",
    "                        \"inits\": [\n",
    "                            1\n",
    "                        ],\n",
    "                        \"name\": \"Signal_norm\"\n",
    "                    }\n",
    "                ],\n",
    "                \"poi\": \"Signal_norm\"\n",
    "            },\n",
    "            \"name\": \"minimal_example\"\n",
    "        }\n",
    "    ],\n",
    "    \"observations\": [\n",
    "        {\n",
    "            \"data\": data_hist[0].tolist(),\n",
    "            \"name\": \"Signal_region\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0.0\"\n",
    "}\n",
    "\n",
    "with open(\"higgs4l_pyhf_workspace.json\", \"w\") as outfile:  \n",
    "    json.dump(pyhf_workspace, outfile, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pyhf Fit\n",
    "\n",
    "After running the fit, extract the best fit normalization factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fit\n",
    "with open(\"higgs4l_pyhf_workspace.json\") as f:\n",
    "    ws = json.load(f)\n",
    "\n",
    "bestfit, uncertainty, labels = fit.fit(ws)\n",
    "\n",
    "for idx, label in enumerate(labels):\n",
    "    if label == 'Signal_norm':\n",
    "        bestfit_norm = bestfit[idx]\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Original and Normalized Signal Region Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stacked_hist(zz_factor=1.3, higgs_factor=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stacked_hist(zz_factor=1.3, higgs_factor=bestfit_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "- Integrate further with Cabinetry framework by using Awkward Arrays instead of Pandas\n",
    "- Add Systematic Uncertainties (maybe dummy)\n",
    "- Simultaneously fit zz normalization factor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
